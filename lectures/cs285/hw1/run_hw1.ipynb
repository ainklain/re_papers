{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bitvenvtorch2condad1393f18b07643de89a53f66f8fef39b",
   "display_name": "Python 3.8.3 64-bit ('venv_torch2': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs285.infrastructure.rl_trainer import RL_Trainer\n",
    "from cs285.agents.bc_agent import BCAgent\n",
    "from cs285.policies.loaded_gaussian_policy import Loaded_Gaussian_Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Requirement already satisfied: easydict in c:\\anaconda3\\envs\\venv_torch2\\lib\\site-packages (1.9)\n"
    }
   ],
   "source": [
    "!pip install easydict\n",
    "\n",
    "import easydict\n",
    "args = easydict.EasyDict({\n",
    "    'expert_policy_file': 'cs285/policies/experts/Ant.pkl',\n",
    "    'expert_data': 'cs285/expert_data/expert_data_Ant-v2.pkl', \n",
    "    'env_name': 'Ant-v2', \n",
    "    'exp_name': 'test_bc_ant',\n",
    "    'do_dagger': False,\n",
    "    'ep_len': 0,\n",
    "    'num_agent_train_steps_per_iter': 1000, \n",
    "    'n_iter': 1, \n",
    "    'batch_size': 1000, \n",
    "    'eval_batch_size': 200, \n",
    "    'train_batch_size': 100, \n",
    "    'n_layers': 2, \n",
    "    'size': 64, \n",
    "    'learning_rate': 5e-3, \n",
    "    'video_log_freq': 5, \n",
    "    'scalar_log_freq': 1, \n",
    "    'use_gpu': True, \n",
    "    'which_gpu': 0, \n",
    "    'max_replay_buffer_size': 1000000, \n",
    "    'seed': 1\n",
    "})\n",
    "params = vars(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BC_Trainer(object):\n",
    "    def __init__(self, params):\n",
    "        agent_params = {\n",
    "            'n_layers': params['n_layers'], \n",
    "            'size': params['size'], \n",
    "            'learning_rate': params['learning_rate'], \n",
    "            'max_replay_buffer_size': params['max_replay_buffer_size'], \n",
    "        }\n",
    "        self.params = params\n",
    "        self.params['agent_class'] = BCAgent\n",
    "        self.params['agent_params'] = agent_params\n",
    "\n",
    "        self.rl_trainer = RL_Trainer(self.params)\n",
    "\n",
    "        print('Loading expert policy from ...', self.params['expert_policy_file'])\n",
    "        self.loaded_expert_policy = Loaded_Gaussian_Policy(self.params['expert_policy_file'])\n",
    "        print('Done restoring expert policy...')\n",
    "\n",
    "    def run_training_loop(self):\n",
    "        self.rl_trainer.run_training_loop(\n",
    "            n_iter=self.params['n_iter'],\n",
    "            initial_expertdata=self.params['expert_data'], \n",
    "            collect_policy=self.rl_trainer.agent.actor, \n",
    "            eval_policy=self.rl_trainer.agent.actor, \n",
    "            relabel_with_expert=self.params['do_dagger'], \n",
    "            expert_policy=self.loaded_expert_policy,\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logdir_prefix = 'bc_'\n",
    "if args.do_dagger:\n",
    "    logdir_prefix = 'dagger_'\n",
    "    assert args.n_iter>1, 'DAGGER needs more than 1 iteration of training, to iteratively query to expert and train'\n",
    "else:\n",
    "    assert args.n_iter == 1, 'Vanilla behavior cloning collects expert data just once'\n",
    "\n",
    "data_path = os.path.join(os.getcwd(), './cs285/data')\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "\n",
    "logdir=logdir_prefix + args.exp_name + '_' + args.env_name + '_' + time.strftime('%Y-%m-%d-%H-%M-%S')\n",
    "logdir=os.path.join(data_path, logdir)\n",
    "params['logdir'] = logdir\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'mean': MLP(\n  (layers): ModuleList(\n    (0): Linear(in_features=28, out_features=64, bias=True)\n    (1): Linear(in_features=64, out_features=64, bias=True)\n    (2): Linear(in_features=64, out_features=8, bias=True)\n  )\n), 'logstd': Parameter containing:\ntensor([0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)}\n"
    }
   ],
   "source": [
    "\n",
    "import gym\n",
    "import pybullet_envs\n",
    "import pybullet as p\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from cs285.infrastructure.torch_utils import MLP\n",
    "\n",
    "p.connect(p.DIRECT)\n",
    "env_name = params['env_name'].split('-')[0] + 'BulletEnv-v0'\n",
    "env = gym.make(env_name)\n",
    "env.seed(params['seed'])\n",
    "env.render()\n",
    "\n",
    "params['ep_len'] = params['ep_len'] or env.spec.max_episode_steps\n",
    "\n",
    "ob_dim = env.observation_space.shape[0]\n",
    "ac_dim = env.action_space.shape[0]\n",
    "\n",
    "params['agent_class'] = BCAgent\n",
    "params['agent_params'] = {'n_layers': params['n_layers'],\n",
    "                          'size': params['size'],\n",
    "                          'learning_rate': params['learning_rate'],\n",
    "                          'max_replay_buffer_size': params              ['max_replay_buffer_size'],}\n",
    "params['agent_params']['ac_dim'] = ac_dim\n",
    "params['agent_params']['ob_dim'] = ob_dim\n",
    "params['agent_params']['discrete'] = False\n",
    "\n",
    "fps = env.env.metadata['video.frames_per_second']\n",
    "\n",
    "agent_class = params['agent_class']\n",
    "\n",
    "mean = MLP(ob_dim, output_size=ac_dim, n_layers=params['n_layers'], size=params['size'])\n",
    "logstd = torch.zeros(ac_dim, dtype=torch.float32, requires_grad=True)\n",
    "a = {'mean':mean, 'logstd': nn.Parameter(logstd)}\n",
    "\n",
    "optimizer = torch.optim.Adam([{'params':a['mean'].parameters(), 'params': a['logstd']}], params['learning_rate'])\n",
    "\n",
    "print(a)\n",
    "\n",
    "\n",
    "agent = agent_class(env, params['agent_params'])\n",
    "policy = agent.actor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[-1.2914,  1.2602, -0.5985,  1.0054, -1.0766, -1.1620, -1.0520,  0.6074]],\n       grad_fn=<AddBackward0>)"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "from cs285.infrastructure.utils import *\n",
    "\n",
    "ob = env.reset()\n",
    "policy = agent.actor\n",
    "# sample_trajectory(env, policy, 300, False)\n",
    "obs = torch.tensor(ob, dtype=torch.float32)\n",
    "policy.parameters['mean'](obs)\n",
    "a = policy.get_action(obs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "########################\nlogging outputs to  d:\\projects\\re_papers\\lectures\\cs285\\hw1\\./cs285/data\\bc_test_bc_ant_Ant-v2_2020-09-01-15-52-47\n########################\nLoading expert policy from ... cs285/policies/experts/Ant.pkl\nDone restoring expert policy...\n\n\n********** Iteration 0 ************\n"
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-05137174fed4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBC_Trainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_training_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-ba196661444b>\u001b[0m in \u001b[0;36mrun_training_loop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrun_training_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         self.rl_trainer.run_training_loop(\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'n_iter'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0minitial_expertdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'expert_data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\projects\\re_papers\\lectures\\cs285\\hw1\\cs285\\infrastructure\\rl_trainer.py\u001b[0m in \u001b[0;36mrun_training_loop\u001b[1;34m(self, n_iter, collect_policy, eval_policy, initial_expertdata, relabel_with_expert, start_relabel_with_expert, expert_policy)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m             \u001b[1;31m# add collected data to replay buffer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_replay_buffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[1;31m# train agent (using sampled data from replay buffer)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\projects\\re_papers\\lectures\\cs285\\hw1\\cs285\\agents\\bc_agent.py\u001b[0m in \u001b[0;36madd_to_replay_buffer\u001b[1;34m(self, paths)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0madd_to_replay_buffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_rollouts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\projects\\re_papers\\lectures\\cs285\\hw1\\cs285\\infrastructure\\replay_buffer.py\u001b[0m in \u001b[0;36madd_rollouts\u001b[1;34m(self, paths, concat_rew)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;31m# convert new rollouts into their component arrays, and append them onto our arrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mobservations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_observations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_listofrollouts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcat_rew\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\projects\\re_papers\\lectures\\cs285\\hw1\\cs285\\infrastructure\\utils.py\u001b[0m in \u001b[0;36mconvert_listofrollouts\u001b[1;34m(paths, concat_rew)\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0mwhere\u001b[0m \u001b[0meach\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mconcatenation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthat\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0macross\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mrollouts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m     \"\"\"\n\u001b[1;32m--> 113\u001b[1;33m     \u001b[0mobservations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"observation\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m     \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"action\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mconcat_rew\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\projects\\re_papers\\lectures\\cs285\\hw1\\cs285\\infrastructure\\utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0mwhere\u001b[0m \u001b[0meach\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mconcatenation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthat\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0macross\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mrollouts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m     \"\"\"\n\u001b[1;32m--> 113\u001b[1;33m     \u001b[0mobservations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"observation\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m     \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"action\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mconcat_rew\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "trainer = BC_Trainer(params)\n",
    "trainer.run_training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}